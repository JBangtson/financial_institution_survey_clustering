{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46150cee",
   "metadata": {},
   "source": [
    "This assignment asks you to cluster data from a credit union in Washington. This data was gathered by survey. I've included a pretty full description of the data as an appendix down below. \n",
    "\n",
    "In class we used code from _Data Science From Scratch_ to cluster, but it's more traditional to use the [`Kmeans` function](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) from `scikit-learn`. I'll show you a brief example of that with the survey data from class, then I ask you to do your own clustering.\n",
    "\n",
    "### Clustering Class Survey Data\n",
    "\n",
    "In class we surveyed students from the MSBA and put them into groups. We'll repeat that work here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e2cab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99b26b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = pd.read_csv(\"survey_responses.txt\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34084471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>What's your family or last name?</th>\n",
       "      <th>Distance from where you were born to UMT</th>\n",
       "      <th>Distance from where you lived at 15 to UMT</th>\n",
       "      <th>How many years have you been in post-secondary school?</th>\n",
       "      <th>Were you an undergraduate marketing major?</th>\n",
       "      <th>Were you an undergraduate business major?</th>\n",
       "      <th>How many people live in your house/apartment (including you)?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/12/2019 21:50:35</td>\n",
       "      <td>Schwartz</td>\n",
       "      <td>1726.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/13/2019 4:18:47</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>2787.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/13/2019 9:24:37</td>\n",
       "      <td>Bankston</td>\n",
       "      <td>1273.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/13/2019 10:57:34</td>\n",
       "      <td>Barr</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11/13/2019 14:50:06</td>\n",
       "      <td>Gabrielsen</td>\n",
       "      <td>276.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Timestamp What's your family or last name?  \\\n",
       "0  11/12/2019 21:50:35                         Schwartz   \n",
       "1   11/13/2019 4:18:47                         Thompson   \n",
       "2   11/13/2019 9:24:37                         Bankston   \n",
       "3  11/13/2019 10:57:34                             Barr   \n",
       "4  11/13/2019 14:50:06                       Gabrielsen   \n",
       "\n",
       "   Distance from where you were born to UMT  \\\n",
       "0                                    1726.0   \n",
       "1                                    2358.0   \n",
       "2                                    1273.0   \n",
       "3                                    1982.0   \n",
       "4                                     276.0   \n",
       "\n",
       "   Distance from where you lived at 15 to UMT  \\\n",
       "0                                       250.0   \n",
       "1                                      2787.0   \n",
       "2                                        45.0   \n",
       "3                                         5.0   \n",
       "4                                       276.0   \n",
       "\n",
       "   How many years have you been in post-secondary school?  \\\n",
       "0                                                9.0        \n",
       "1                                                6.0        \n",
       "2                                                6.0        \n",
       "3                                                7.0        \n",
       "4                                                6.0        \n",
       "\n",
       "  Were you an undergraduate marketing major?   \\\n",
       "0                                          No   \n",
       "1                                          No   \n",
       "2                                         Yes   \n",
       "3                                          No   \n",
       "4                                          No   \n",
       "\n",
       "  Were you an undergraduate business major?  \\\n",
       "0                                        No   \n",
       "1                                       Yes   \n",
       "2                                       Yes   \n",
       "3                                        No   \n",
       "4                                       Yes   \n",
       "\n",
       "   How many people live in your house/apartment (including you)?   \n",
       "0                                                  1               \n",
       "1                                                  2               \n",
       "2                                                  2               \n",
       "3                                                  1               \n",
       "4                                                  3               "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b917a634",
   "metadata": {},
   "source": [
    "As in class, let's convert the data to numeric and we'll do some renaming as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3d5c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_names = example_data.columns\n",
    "new_names = \"timestamp name birth_dist age_15_dist school_years ug_mkt ug_biz hh_size\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcb0a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = example_data.rename(columns = dict(zip(current_names,new_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33f2d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.MySeriesName.map(dict(yes=1, no=0))\n",
    "example_data.ug_mkt = example_data.ug_mkt.map(dict(Yes=1,No=0))\n",
    "example_data.ug_biz = example_data.ug_biz.map(dict(Yes=1,No=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "979a6cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>birth_dist</th>\n",
       "      <th>age_15_dist</th>\n",
       "      <th>school_years</th>\n",
       "      <th>ug_mkt</th>\n",
       "      <th>ug_biz</th>\n",
       "      <th>hh_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/12/2019 21:50:35</td>\n",
       "      <td>Schwartz</td>\n",
       "      <td>1726.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/13/2019 4:18:47</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>2787.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/13/2019 9:24:37</td>\n",
       "      <td>Bankston</td>\n",
       "      <td>1273.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/13/2019 10:57:34</td>\n",
       "      <td>Barr</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11/13/2019 14:50:06</td>\n",
       "      <td>Gabrielsen</td>\n",
       "      <td>276.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp        name  birth_dist  age_15_dist  school_years  \\\n",
       "0  11/12/2019 21:50:35    Schwartz      1726.0        250.0           9.0   \n",
       "1   11/13/2019 4:18:47    Thompson      2358.0       2787.0           6.0   \n",
       "2   11/13/2019 9:24:37    Bankston      1273.0         45.0           6.0   \n",
       "3  11/13/2019 10:57:34        Barr      1982.0          5.0           7.0   \n",
       "4  11/13/2019 14:50:06  Gabrielsen       276.0        276.0           6.0   \n",
       "\n",
       "   ug_mkt  ug_biz  hh_size  \n",
       "0       0       0        1  \n",
       "1       0       1        2  \n",
       "2       1       1        2  \n",
       "3       0       0        1  \n",
       "4       0       1        3  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2af4d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = \"birth_dist age_15_dist school_years ug_mkt ug_biz hh_size\".split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f65f6",
   "metadata": {},
   "source": [
    "This operation is typically done to isolate the numeric data that will be used for further analysis or processing, such as normalization or clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67347522",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_example_data = example_data.loc[:,numeric_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3e2d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example_normalized = (num_example_data - num_example_data.min()) / (num_example_data.max() - num_example_data.min())\n",
    "\n",
    "# Calculate the mean and standard deviation for each column\n",
    "mean_values = num_example_data.mean()\n",
    "std_values = num_example_data.std()\n",
    "\n",
    "# Perform z-score normalization\n",
    "example_normalized = (num_example_data - mean_values) / std_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5208017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birth_dist</th>\n",
       "      <th>age_15_dist</th>\n",
       "      <th>school_years</th>\n",
       "      <th>ug_mkt</th>\n",
       "      <th>ug_biz</th>\n",
       "      <th>hh_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.092412</td>\n",
       "      <td>-0.546875</td>\n",
       "      <td>0.557685</td>\n",
       "      <td>-0.422282</td>\n",
       "      <td>-1.069687</td>\n",
       "      <td>-1.150927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.409156</td>\n",
       "      <td>0.886745</td>\n",
       "      <td>-0.274840</td>\n",
       "      <td>-0.422282</td>\n",
       "      <td>0.916875</td>\n",
       "      <td>-0.295953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.134621</td>\n",
       "      <td>-0.662717</td>\n",
       "      <td>-0.274840</td>\n",
       "      <td>2.322548</td>\n",
       "      <td>0.916875</td>\n",
       "      <td>-0.295953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.220713</td>\n",
       "      <td>-0.685321</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>-0.422282</td>\n",
       "      <td>-1.069687</td>\n",
       "      <td>-1.150927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.634294</td>\n",
       "      <td>-0.532183</td>\n",
       "      <td>-0.274840</td>\n",
       "      <td>-0.422282</td>\n",
       "      <td>0.916875</td>\n",
       "      <td>0.559022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.438265</td>\n",
       "      <td>0.623981</td>\n",
       "      <td>-0.274840</td>\n",
       "      <td>-0.422282</td>\n",
       "      <td>-1.069687</td>\n",
       "      <td>-1.150927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.772117</td>\n",
       "      <td>-0.685321</td>\n",
       "      <td>0.835193</td>\n",
       "      <td>2.322548</td>\n",
       "      <td>0.916875</td>\n",
       "      <td>0.559022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.020854</td>\n",
       "      <td>-0.654241</td>\n",
       "      <td>-0.552348</td>\n",
       "      <td>2.322548</td>\n",
       "      <td>0.916875</td>\n",
       "      <td>-0.295953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.636298</td>\n",
       "      <td>-0.534443</td>\n",
       "      <td>-0.552348</td>\n",
       "      <td>-0.422282</td>\n",
       "      <td>0.916875</td>\n",
       "      <td>-0.295953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.599712</td>\n",
       "      <td>-0.493192</td>\n",
       "      <td>-0.274840</td>\n",
       "      <td>-0.422282</td>\n",
       "      <td>0.916875</td>\n",
       "      <td>-1.150927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   birth_dist  age_15_dist  school_years    ug_mkt    ug_biz   hh_size\n",
       "0    0.092412    -0.546875      0.557685 -0.422282 -1.069687 -1.150927\n",
       "1    0.409156     0.886745     -0.274840 -0.422282  0.916875 -0.295953\n",
       "2   -0.134621    -0.662717     -0.274840  2.322548  0.916875 -0.295953\n",
       "3    0.220713    -0.685321      0.002668 -0.422282 -1.069687 -1.150927\n",
       "4   -0.634294    -0.532183     -0.274840 -0.422282  0.916875  0.559022\n",
       "5    3.438265     0.623981     -0.274840 -0.422282 -1.069687 -1.150927\n",
       "6   -0.772117    -0.685321      0.835193  2.322548  0.916875  0.559022\n",
       "7   -0.020854    -0.654241     -0.552348  2.322548  0.916875 -0.295953\n",
       "8   -0.636298    -0.534443     -0.552348 -0.422282  0.916875 -0.295953\n",
       "9   -0.599712    -0.493192     -0.274840 -0.422282  0.916875 -1.150927"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_normalized[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aaf227",
   "metadata": {},
   "source": [
    "Let's set up our K-Means model. Some of the below code is adapted from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b03b1a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(\n",
    "    n_clusters=4, \n",
    "    init='random',\n",
    "    n_init=10, \n",
    "    max_iter=300, \n",
    "    tol=1e-04, \n",
    "    random_state=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95473527",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = km.fit_predict(example_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b31bc2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data['cluster'] = clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "483485b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              timestamp         name  birth_dist  age_15_dist  school_years  \\\n",
      "0    11/13/2019 4:18:47     Thompson      2358.0       2787.0           6.0   \n",
      "1   11/13/2019 14:50:06   Gabrielsen       276.0        276.0           6.0   \n",
      "2   11/13/2019 16:03:36    Cerkovnik       272.0        272.0           5.0   \n",
      "3   11/13/2019 16:03:48        McNea       345.0        345.0           6.0   \n",
      "4   11/13/2019 16:04:18       Toepke       543.0        840.0           6.0   \n",
      "5    11/3/2020 11:59:47    Hendricks        32.0         32.0           5.0   \n",
      "6    11/3/2020 14:06:10     Anderson       700.0        700.0           5.0   \n",
      "7    11/3/2020 16:48:11        Moore       871.0        892.0           8.0   \n",
      "8    11/5/2020 17:23:28      Gilbert      1579.0       1579.0           2.0   \n",
      "9    11/9/2020 10:26:20    Stahlberg        95.0         95.0           7.0   \n",
      "10   11/9/2020 10:30:12      Hoffman       272.0        272.0           6.0   \n",
      "11   11/1/2021 17:56:58    Stahlberg       120.0        120.0           8.0   \n",
      "12   11/8/2021 16:08:47       Joyner       276.0        583.0           6.0   \n",
      "13   11/8/2021 16:09:49    Hettinger      1157.0          3.0           5.0   \n",
      "14   11/7/2022 16:05:05        Fanok      2303.0       2303.0           5.0   \n",
      "15   11/7/2022 16:05:24   Dauenhauer         3.0          1.0           5.0   \n",
      "16   11/7/2022 16:05:51       Wagers       549.0        658.0           5.0   \n",
      "17  11/27/2023 17:50:52        Keith       932.0        952.0           8.0   \n",
      "18  10/21/2024 17:17:18      Haefele         0.7          0.5           6.0   \n",
      "19   11/13/2019 9:24:37     Bankston      1273.0         45.0           6.0   \n",
      "20  11/13/2019 16:02:42      Leonard         1.0          5.0          10.0   \n",
      "21  11/13/2019 16:03:33       Martin      1500.0         60.0           5.0   \n",
      "22  11/13/2019 16:04:03       Runkel         5.0         56.0           5.0   \n",
      "23   11/2/2020 21:12:19     Danicich       478.0          5.0           5.0   \n",
      "24   11/6/2020 19:40:50     Scheibel       202.0          0.0           5.0   \n",
      "25   11/7/2022 16:06:36    Halderman      1782.0       1906.0           5.0   \n",
      "26  11/27/2023 19:50:48     Thompson       202.0        202.0           6.0   \n",
      "27  11/13/2019 16:01:33    Campestre      8402.0       2322.0           6.0   \n",
      "28  11/13/2019 16:05:42     Bernhard      4505.0       4514.0           5.0   \n",
      "29   11/7/2022 16:04:49     Knowlton      2571.0       2580.0          23.0   \n",
      "30   11/7/2022 16:07:50           Bu      6756.0       6756.0          12.0   \n",
      "31  11/29/2023 17:10:59  Bosco Louis      8279.0       8279.0           5.0   \n",
      "32  11/30/2023 16:35:05    Ghazouani      5700.0       5700.0           7.0   \n",
      "33  11/12/2019 21:50:35     Schwartz      1726.0        250.0           9.0   \n",
      "34  11/13/2019 10:57:34         Barr      1982.0          5.0           7.0   \n",
      "35  11/13/2019 16:04:47       Stokes       166.0        216.0           9.0   \n",
      "36   11/2/2020 18:58:40     Williams      2400.0       2350.0          13.0   \n",
      "37    11/3/2020 7:50:52       Fowler      1500.0       1500.0           8.0   \n",
      "38   11/3/2020 16:59:53       Howell         2.0         60.0          21.0   \n",
      "39   11/3/2020 18:08:10        Hauer      1650.0         70.0           8.0   \n",
      "40   11/3/2021 10:46:13     Phillips       723.0         12.0           6.0   \n",
      "41   11/7/2021 20:04:13       Connor      1616.0       1606.0           5.0   \n",
      "42   11/9/2021 18:42:48       Nelson      2264.0       2264.0           6.0   \n",
      "43   11/7/2022 16:05:19        Albee      1012.0       1023.0           6.0   \n",
      "44   11/7/2022 16:13:01    Robertson      2000.0       2500.0           7.0   \n",
      "45  11/27/2023 16:42:59     Whattam        342.0        344.0           4.0   \n",
      "46  11/30/2023 11:23:43         Paul       200.0        200.0           7.0   \n",
      "47  10/21/2024 12:31:44    Engellant      4814.0         92.8           4.5   \n",
      "48  10/21/2024 16:07:28        Woods       901.0        903.0           5.0   \n",
      "49  10/21/2024 16:08:25      Niekamp       875.0        875.0          10.0   \n",
      "50  10/21/2024 16:14:51         Daly       731.0       2993.0           7.0   \n",
      "51  10/21/2024 16:22:31      Parrent       920.0        920.0           6.0   \n",
      "\n",
      "    ug_mkt  ug_biz  hh_size  cluster  \n",
      "0        0       1        2        0  \n",
      "1        0       1        3        0  \n",
      "2        0       1        2        0  \n",
      "3        0       1        1        0  \n",
      "4        0       1        1        0  \n",
      "5        0       1        4        0  \n",
      "6        0       1        2        0  \n",
      "7        0       1        4        0  \n",
      "8        0       1        4        0  \n",
      "9        0       1        2        0  \n",
      "10       0       1        4        0  \n",
      "11       0       1        2        0  \n",
      "12       0       1        2        0  \n",
      "13       0       1        2        0  \n",
      "14       0       1        2        0  \n",
      "15       0       1        3        0  \n",
      "16       0       1        3        0  \n",
      "17       0       1        5        0  \n",
      "18       0       1        2        0  \n",
      "19       1       1        2        1  \n",
      "20       1       1        3        1  \n",
      "21       1       1        2        1  \n",
      "22       1       1        2        1  \n",
      "23       1       1        2        1  \n",
      "24       1       1        2        1  \n",
      "25       1       1        2        1  \n",
      "26       1       1        2        1  \n",
      "27       0       0        1        2  \n",
      "28       0       0        2        2  \n",
      "29       0       0        1        2  \n",
      "30       0       0        2        2  \n",
      "31       0       1        1        2  \n",
      "32       0       0        4        2  \n",
      "33       0       0        1        3  \n",
      "34       0       0        1        3  \n",
      "35       0       0        3        3  \n",
      "36       0       0        2        3  \n",
      "37       0       0        2        3  \n",
      "38       0       0        1        3  \n",
      "39       0       0        7        3  \n",
      "40       0       0        2        3  \n",
      "41       0       0        2        3  \n",
      "42       0       0        4        3  \n",
      "43       0       0        3        3  \n",
      "44       0       0        3        3  \n",
      "45       0       0        2        3  \n",
      "46       0       0        2        3  \n",
      "47       0       0        2        3  \n",
      "48       0       0        1        3  \n",
      "49       0       0        2        3  \n",
      "50       0       0        1        3  \n",
      "51       0       0        3        3  \n"
     ]
    }
   ],
   "source": [
    "cluster_df = pd.DataFrame()\n",
    "\n",
    "for cluster in set(clustering):\n",
    "    cluster_data = example_data[example_data['cluster'] == cluster]\n",
    "    cluster_df = pd.concat([cluster_df, cluster_data], axis=0)\n",
    "\n",
    "cluster_df = cluster_df.reset_index(drop=True)\n",
    "\n",
    "print(cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c6ae0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cluster_df to csv\n",
    "cluster_df.to_csv('data/survey_cluster_results_fourgroup.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b83dc",
   "metadata": {},
   "source": [
    "There's much more we could do here, such as printing out the cluster means. With this current scaling approach it appears that we're basically splitting into UG degrees. It might be more interesting to normalize by subtracting the mean of the column and dividing by the standard deviation. \n",
    "\n",
    "## Demographic Clustering\n",
    "\n",
    "In this section, perform a cluster analysis using the fields unrelated to the values survey:\n",
    "\n",
    "* age\n",
    "* gender\n",
    "* engagement\n",
    "* account.age\n",
    "\n",
    "Choose a number of clusters and use K-means to cluster the data using these fields. Briefly describe the clusters. How evenly do your clusters align with the regions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e97f5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your work here\n",
    "demographic_data = pd.read_csv(\"washington_survey_data.txt\",sep=\"\\t\")\n",
    "num_demo_data = demographic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb7fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = \"age gender engagement account.age\".split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28048f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_demographic_data = demographic_data.loc[:,numeric_cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a65ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c23d3c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_demographic_data.gender = num_demographic_data.gender.map(dict(other=2,female=1,male=0))\n",
    "num_demographic_data.engagement = num_demographic_data.engagement.map({\"Highly Engaged\" : 2,\"Engaged\": 1, \"Not Engaged\": 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a87d108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example_normalized = (num_example_data - num_example_data.min()) / (num_example_data.max() - num_example_data.min())\n",
    "\n",
    "# Calculate the mean and standard deviation for each column\n",
    "mean_values = num_demographic_data.mean()\n",
    "std_values = num_demographic_data.std()\n",
    "\n",
    "# Perform z-score normalization\n",
    "demo_normalized = (num_demographic_data - mean_values) / std_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d51d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(\n",
    "    n_clusters=4, \n",
    "    init='random',\n",
    "    n_init=10, \n",
    "    max_iter=300, \n",
    "    tol=1e-04, \n",
    "    random_state=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4e095a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = km.fit_predict(demo_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bac6ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_normalized['cluster'] = clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88c5ca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           age    gender  engagement  account.age  cluster\n",
      "0     0.331628  2.349315   -0.059365    -0.327557        3\n",
      "1     0.951989  0.552256    1.179627    -0.421563        0\n",
      "2     1.262170 -1.244803   -1.298358    -0.715660        2\n",
      "3     0.951989 -1.244803   -0.059365    -0.391664        2\n",
      "4    -0.040589 -1.244803    1.179627    -0.190813        0\n",
      "...        ...       ...         ...          ...      ...\n",
      "2416  2.006603  0.552256   -0.059365    -0.968539        3\n",
      "2417  2.626965  0.552256   -0.059365     2.515685        1\n",
      "2418 -0.350769  0.552256   -1.298358     0.583703        3\n",
      "2419 -1.095203 -1.244803   -0.059365    -0.868367        2\n",
      "2420 -1.467419  0.552256   -1.298358    -1.150470        3\n",
      "\n",
      "[2421 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "cluster_df = pd.DataFrame(demo_normalized)\n",
    "cluster_df['cluster'] = clustering\n",
    "\n",
    "print(cluster_df)\n",
    "# Your work here\n",
    "demographic_data = num_demo_data\n",
    "demographic_data['cluster'] = cluster_df['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "786b2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cluster_df to csv\n",
    "demographic_data.to_csv('data/demo_cluster_results_fourgroup.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d4d12",
   "metadata": {},
   "source": [
    "## Values Clustering\n",
    "\n",
    "Similarly to the previous section, perform a cluster analysis, this time on the values questions:\n",
    "\n",
    "* fair\n",
    "* harm\n",
    "* in.group\n",
    "* authority\n",
    "* purity\n",
    "* public.sector\n",
    "* sustainability\n",
    "* localism\n",
    "\n",
    "After youâ€™ve built your clusters, report the following information on each cluster:\n",
    "\n",
    "* Predominant region\n",
    "* Average age and account age\n",
    "* Most common focal value\n",
    "* Mean results on the questions of `pub.greater.priv`, `experience.more.important`, and `teachers.underpaid`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "858176dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_data = pd.read_csv(\"washington_survey_data.txt\",sep=\"\\t\")\n",
    "num_demo_data = demographic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9452f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = \"fair harm in.group authority purity public.sector sustainability localism\".split()\n",
    "num_demographic_data = demographic_data.loc[:,numeric_cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b71152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming num_demographic_data is your DataFrame\n",
    "num_demographic_data['public.sector'] = num_demographic_data['public.sector'].map({\"yes\": 1, \"no\": 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ffede0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example_normalized = (num_example_data - num_example_data.min()) / (num_example_data.max() - num_example_data.min())\n",
    "\n",
    "# Calculate the mean and standard deviation for each column\n",
    "mean_values = num_demographic_data.mean()\n",
    "std_values = num_demographic_data.std()\n",
    "\n",
    "# Perform z-score normalization\n",
    "demo_normalized = (num_demographic_data - mean_values) / std_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffdd8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(\n",
    "    n_clusters=5, \n",
    "    init='random',\n",
    "    n_init=10, \n",
    "    max_iter=300, \n",
    "    tol=1e-04, \n",
    "    random_state=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b60d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = km.fit_predict(demo_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcc36069",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_normalized['cluster'] = clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "991657c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          fair      harm  in.group  authority    purity  public.sector  \\\n",
      "0    -3.866462 -3.784118 -2.165151  -1.902780 -1.380514       0.853083   \n",
      "1     1.168521  1.400023 -0.889021   0.293643 -0.527144       0.853083   \n",
      "2     0.497190 -0.868039 -0.889021   0.293643 -0.100459       0.853083   \n",
      "3     1.168521  0.427997  1.663239   0.293643  0.752912      -1.171735   \n",
      "4     0.832855 -2.488083  0.387109   1.513878  0.326227       0.853083   \n",
      "...        ...       ...       ...        ...       ...            ...   \n",
      "2416  0.832855  1.076015  1.408013  -0.194451  0.326227       0.853083   \n",
      "2417  0.497190  1.076015 -0.378569  -0.682545  0.752912      -1.171735   \n",
      "2418 -0.174141  0.103988 -0.378569   0.049596 -0.527144       0.853083   \n",
      "2419  1.168521  1.400023  1.918465   1.025784  1.819625       0.853083   \n",
      "2420  0.161524  0.103988  0.131883  -0.926592 -0.953829      -1.171735   \n",
      "\n",
      "      sustainability  localism  cluster  \n",
      "0          -1.950921 -2.300722        3  \n",
      "1           1.635396  1.200985        0  \n",
      "2           0.820324 -0.258060        0  \n",
      "3           1.146353 -0.258060        4  \n",
      "4          -1.461878 -0.841678        1  \n",
      "...              ...       ...      ...  \n",
      "2416       -0.646806 -1.133487        2  \n",
      "2417        0.820324  1.200985        4  \n",
      "2418        0.168266  0.909176        0  \n",
      "2419        0.820324  1.492794        2  \n",
      "2420        1.146353  0.617367        0  \n",
      "\n",
      "[2421 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "cluster_df = pd.DataFrame(demo_normalized)\n",
    "cluster_df['cluster'] = clustering\n",
    "\n",
    "print(cluster_df)\n",
    "# Your work here\n",
    "#demographic_data = num_demo_data\n",
    "demographic_data['cluster'] = cluster_df['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebbece28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cluster_df to csv\n",
    "demographic_data.to_csv('data/values_cluster_results_fivegroup.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310eaef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386d05ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386a0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac962965",
   "metadata": {},
   "source": [
    "##  Appendix: Full Data Description\n",
    "A financial institution in Washington has become concerned that their current membership \n",
    "base is not well-aligned with their corporate values. Through that concern they \n",
    "realized that don't actually understand their membership's values very well. They \n",
    "surveyed 2,421 members to shed light on the issue. \n",
    "\n",
    "The heart of the survey was the Moral Foundations Theory of Jonathan Haidt. Members \n",
    "were surveyed on the Moral Foundations Questionnaire, which you should take so you \n",
    "understand the test. Survey respondents were scored on the five foundations as well \n",
    "as a single-number summary, Progressivism. \n",
    "\n",
    "The financial institution values Localism, Sustainability, and Education. These aspects \n",
    "of member's values were assessed in the survey as well. Localism and Sustainability used\n",
    "validated scales and thus can be summarized via a single score, where higher values indicate\n",
    "greater support for the values. Education is summarized by the following three questions, \n",
    "which we do not have evidence can be combined into a single score:\n",
    "\n",
    "* In general, public schools provide a better education than private schools.\n",
    "* Public school teachers are underpaid.\n",
    "* Experience is more important than education in determining success in life.\n",
    "These questions were evaluated on a 1 to 6 scale where 1 indicated \"Strongly Disagree\" and \n",
    "6 indicated \"Strongly Agree\". \n",
    "\n",
    "Finally, we have information on the member that can be used to understand variation \n",
    "in their values. \n",
    "\n",
    "The data consists of the following columns:\n",
    "\n",
    "* ID: a unique identifier for the survey respondent.\n",
    "* age: the age of the respondent.\n",
    "* gender: gender was evaluated with robust scale and collapsed into male/female/other for \n",
    "  those whose gender identity was not male or female.\n",
    "* engagement: three categories of engagement with the financial institution.\n",
    "* mem.edu: the self-reported education level of the member with the following scale:\n",
    "* zip: the member zip code. \n",
    "* channel: how the member joined the financial institution. Options are \"Loan\" if they joined \n",
    "  via an auto loan, \"Branch\" if they joined at a branch and other for online or unknown. \n",
    "* progressivism/harm/fair/in.group/authority/purity: The MFQ results.\n",
    "* account.age: the age of the member's account, in years. \n",
    "* region: The region of Washington the member lives in. May be easier to work with than zip.\n",
    "* public.sector: has the person ever been a public employee?\n",
    "* sustainability/localism: Scores on the validated scales. Higher values indicate greater\n",
    "  support for the value.\n",
    "* pub.greater.priv/experience.more.important/teachers.underpaid: The responses to the \n",
    "  education questions above. \n",
    "* main.focal.value: Respondents were asked, \"Below is a list of broad areas to which people \n",
    "  often dedicate their volunteer or philanthropic efforts. From this list, please select the \n",
    "  most important to you. If an area of particular importance is missing, please let us know \n",
    "  about it in the space for 'other.'\" This column holds the respondents' answer to that question.\n",
    "* support.of.focal.value: Respondents were given an opportunity to indicate how they \n",
    "  supported their focal value. Those responses were collapsed into a single score, where \n",
    "  a higher value indicates more support.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
